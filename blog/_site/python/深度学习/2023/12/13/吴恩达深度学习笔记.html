<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>吴恩达深度学习笔记</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="吴恩达深度学习笔记" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="吴恩达深度学习笔记" />
<meta property="og:description" content="吴恩达深度学习笔记" />
<link rel="canonical" href="http://localhost:4000/blog/python/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2023/12/13/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html" />
<meta property="og:url" content="http://localhost:4000/blog/python/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2023/12/13/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-12-13T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="吴恩达深度学习笔记" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-12-13T00:00:00+08:00","datePublished":"2023-12-13T00:00:00+08:00","description":"吴恩达深度学习笔记","headline":"吴恩达深度学习笔记","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/python/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2023/12/13/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html"},"url":"http://localhost:4000/blog/python/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2023/12/13/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/blog/feed.xml" /></head>
<body><a href="/" class="home-link">返回简历主页</a>
<!-- 确保自定义样式被加载 -->
<link rel="stylesheet" href="/blog/assets/css/main.scss"><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">吴恩达深度学习笔记</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2023-12-13T00:00:00+08:00" itemprop="datePublished">Dec 13, 2023
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="吴恩达深度学习笔记">吴恩达深度学习笔记</h1>

<h1 id="第一门课-神经网络和深度学习neural-networks-and-deep-learning">第一门课 神经网络和深度学习(Neural Networks and Deep Learning)</h1>

<h2 id="第一周深度学习引言introduction-to-deep-learning">第一周：深度学习引言(Introduction to Deep Learning)</h2>
<h3 id="11-欢迎welcome">1.1 欢迎(Welcome)</h3>
<p>深度学习改变传统互联网业务，如网络搜索和广告，还催生新产品和企业，在读取X光图像、个性化教育、精准农业、自动驾驶等领域表现出色。本课程是专项课程的第一门，后续还有四门。</p>
<ul>
  <li><strong>课程安排</strong>
    <ul>
      <li><strong>第一门课（神经网络和深度学习）</strong>：四周，学习神经网络基础，建立深度神经网络并在数据上训练，结尾用深度神经网络辨认猫。</li>
      <li><strong>第二门课</strong>：三周，进行深度学习实践，学习超参数调整、正则化、诊断偏差和方差以及高级优化算法（如Momentum和Adam算法）。</li>
      <li><strong>第三门课</strong>：两周，学习如何结构化机器学习工程，如数据分割方式、端对端深度学习等。</li>
      <li><strong>第四门课</strong>：学习卷积神经网络（CNN）搭建。</li>
      <li><strong>第五门课</strong>：学习序列模型（如RNN、LSTM）及其在自然语言处理等问题中的应用。</li>
    </ul>
  </li>
</ul>

<h3 id="13-神经网络的监督学习supervised-learning-with-neural-networks">1.3 神经网络的监督学习(Supervised Learning with Neural Networks)</h3>
<p>深度学习系统通过选择合适的 $x$ 和 $y$ 拟合监督学习部分，不同类型的神经网络适用于不同应用：</p>
<ul>
  <li><strong>标准神经网络</strong>：用于房地产和在线广告等相对标准的应用。</li>
  <li><strong>卷积神经网络（CNN）</strong>：常用于图像应用。</li>
  <li><strong>递归神经网络（RNN）</strong>：适用于序列数据，如音频、语言等。对于更复杂的应用（如自动驾驶），可能会使用定制或混合的神经网络结构。</li>
</ul>

<h2 id="第二周神经网络的编程基础basics-of-neural-network-programming">第二周：神经网络的编程基础(Basics of Neural Network programming)</h2>
<h3 id="21-二分类binary-classification">2.1 二分类(Binary Classification)</h3>
<ul>
  <li><strong>训练集表示</strong>：定义矩阵 $X$ 由输入向量 $x^{(1)},x^{(2)},\cdots,x^{(m)}$ 按列组成，规模为 $n_x \times m$，即 <code class="language-plaintext highlighter-rouge">X.shape</code> 等于 $(n_x,m)$。输出标签 $Y$ 由 $y^{(1)},y^{(2)},\cdots,y^{(m)}$ 按列组成，规模为 $1 \times m$，即 <code class="language-plaintext highlighter-rouge">Y.shape</code> 等于 $(1,m)$。</li>
</ul>

<h3 id="24-梯度下降法gradient-descent">2.4 梯度下降法（Gradient Descent）</h3>
<ul>
  <li><strong>步骤</strong>
    <ol>
      <li>以小红点坐标初始化参数 $w$ 和 $b$。</li>
      <li>朝最陡的下坡方向走一步，不断迭代。</li>
      <li>直到走到全局最优解或者接近全局最优解的地方。</li>
    </ol>
  </li>
  <li><strong>细节说明（仅有一个参数）</strong>：假定代价函数 $J(w)$ 只有一个参数 $w$，迭代公式为：
    <ul>
      <li>$w := w - \alpha \frac{dJ(w)}{dw}$</li>
      <li>其中，$\alpha$ 表示学习率，控制步长；$\frac{dJ(w)}{dw}$ 是函数 $J(w)$ 对 $w$ 的导数，代码中用 $dw$ 表示。</li>
    </ul>
  </li>
</ul>

<h3 id="210--m-个样本的梯度下降gradient-descent-on-m-examples">2.10  m 个样本的梯度下降(Gradient Descent on m Examples)</h3>
<ul>
  <li><strong>代价函数</strong>：$J(w,b)=\frac{1}{m}\sum\limits_{i=1}^{m}{L(^{(i)},^{(i)})}$</li>
  <li><strong>代码流程</strong>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">J</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">dw1</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">dw2</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">db</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
  <span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">;</span>
  <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
  <span class="n">J</span> <span class="o">+=</span> <span class="o">-</span> <span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="nf">log</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="nf">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]));</span>
  <span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
  <span class="n">dw1</span> <span class="o">+=</span> <span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
  <span class="n">dw2</span> <span class="o">+=</span> <span class="n">x2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
  <span class="n">db</span> <span class="o">+=</span> <span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="n">J</span> <span class="o">/=</span> <span class="n">m</span><span class="p">;</span>
<span class="n">dw1</span> <span class="o">/=</span> <span class="n">m</span><span class="p">;</span>
<span class="n">dw2</span> <span class="o">/=</span> <span class="n">m</span><span class="p">;</span>
<span class="n">db</span> <span class="o">/=</span> <span class="n">m</span><span class="p">;</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dw1</span><span class="p">;</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">db</span><span class="p">;</span>
</code></pre></div>    </div>
    <p>这种方法有两个缺点，需要编写两个 <code class="language-plaintext highlighter-rouge">for</code> 循环，深度学习中显式使用 <code class="language-plaintext highlighter-rouge">for</code> 循环会使算法低效，可使用向量化技术摆脱显式 <code class="language-plaintext highlighter-rouge">for</code> 循环。</p>
  </li>
</ul>

<h3 id="217-jupyteripython-notebooks快速入门quick-tour-of-jupyteripython-notebooks">2.17 Jupyter/iPython Notebooks快速入门（Quick tour of Jupyter/iPython Notebooks）</h3>
<ul>
  <li><strong>界面与代码块</strong>：通过界面连接到Coursera，有空白代码块可编写代码，代码块中有开始代码和结束代码，编程练习时代码写在两者之间。</li>
  <li><strong>运行代码块</strong>：可按 <code class="language-plaintext highlighter-rouge">shift + enter</code> 运行代码块，也可点击 <code class="language-plaintext highlighter-rouge">Cell</code> 菜单的 <code class="language-plaintext highlighter-rouge">Run Cells</code> 执行部分代码。</li>
  <li><strong>恢复格式</strong>：若不小心将区域变成markdown语言形式，运行单元格（<code class="language-plaintext highlighter-rouge">shift + enter</code> 或点击 <code class="language-plaintext highlighter-rouge">Cell</code> 菜单的 <code class="language-plaintext highlighter-rouge">Run Cells</code>）可恢复。</li>
  <li><strong>重启内核</strong>：若出现错误信息（如Kernel中断），可点击 <code class="language-plaintext highlighter-rouge">Kernel</code>，选择 <code class="language-plaintext highlighter-rouge">Restart</code> 重启内核。</li>
  <li><strong>提交作业</strong>：完成作业后，点击右上方蓝色的 <code class="language-plaintext highlighter-rouge">Submit Assignment</code> 按钮提交。</li>
</ul>

<h2 id="第三周浅层神经网络shallow-neural-networks">第三周：浅层神经网络(Shallow neural networks)</h2>
<h3 id="31-神经网络概述neural-network-overview">3.1 神经网络概述（Neural Network Overview）</h3>
<ul>
  <li><strong>逻辑回归回顾</strong>：输入特征 $x$、参数 $w$ 和 $b$，计算 $z = w^T x + b$，再通过激活函数 $\sigma(z)$ 得到 $a$，最后计算损失函数 $L(a,y)$。</li>
  <li><strong>神经网络结构</strong>：由多个sigmoid单元堆叠而成，使用符号 $^{[m]}$ 表示第 $m$ 层网络中节点相关的数，与表示单个训练样本的 $^{(i)}$ 区分。</li>
</ul>

<h3 id="32-多样本向量化vectorizing-across-multiple-examples">3.2 多样本向量化（Vectorizing across multiple examples）</h3>
<ul>
  <li><strong>非向量化实现</strong>：对于 $m$ 个训练样本，需重复计算预测值，方程如下：
    <ul>
      <li>$z^{<a href="i">1</a>}=W^{<a href="i">1</a>}x^{(i)}+b^{<a href="i">1</a>}$</li>
      <li>$a^{<a href="i">1</a>}=\sigma(z^{<a href="i">1</a>})$</li>
      <li>$z^{<a href="i">2</a>}=W^{<a href="i">2</a>}a^{<a href="i">1</a>}+b^{<a href="i">2</a>}$</li>
      <li>$a^{<a href="i">2</a>}=\sigma(z^{<a href="i">2</a>})$</li>
    </ul>
  </li>
  <li><strong>向量化实现</strong>：将训练样本组合成矩阵 $X$（规模为 $n \times m$），同理得到矩阵 $Z^{[1]}$、$A^{[1]}$、$Z^{[2]}$ 和 $A^{[2]}$。水平索引对应不同训练样本，垂直索引对应神经网络不同节点。</li>
</ul>

<h1 id="第二门课-改善深层神经网络超参数调试正则化以及优化improving-deep-neural-networkshyperparameter-tuning-regularization-and-optimization">第二门课 改善深层神经网络：超参数调试、正则化以及优化(Improving Deep Neural Networks:Hyperparameter tuning, Regularization and Optimization)</h1>

<h2 id="第一周深度学习的实践层面practical-aspects-of-deep-learning">第一周：深度学习的实践层面(Practical aspects of Deep Learning)</h2>
<h3 id="16-dropout-正则化dropout-regularization">1.6 dropout 正则化（Dropout Regularization）</h3>
<h4 id="实施方法">实施方法</h4>
<p>实施 <strong>dropout</strong> 最常用的方法是 <strong>inverted dropout</strong>（反向随机失活），下面以一个三层（$l = 3$）网络为例说明如何在某一层中实施 <strong>dropout</strong>：</p>
<ol>
  <li>定义向量 $d$，$d^{[3]}$ 表示一个三层的 <strong>dropout</strong> 向量：
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">d3</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">a3</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">a3</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div>    </div>
  </li>
  <li>设定 <strong>keep-prob</strong>，它表示保留某个隐藏单元的概率，例如设为 0.8，意味着消除任意一个隐藏单元的概率是 0.2。判断 $d^{[3]}$ 中的元素是否小于 <strong>keep-prob</strong>：
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">keep_prob</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">d3</span> <span class="o">=</span> <span class="p">(</span><span class="n">d3</span> <span class="o">&lt;</span> <span class="n">keep_prob</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>从第三层中获取激活函数 $a^{[3]}$，将其与 $d^{[3]}$ 进行元素相乘：
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a3</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">multiply</span><span class="p">(</span><span class="n">a3</span><span class="p">,</span> <span class="n">d3</span><span class="p">)</span>
</code></pre></div>    </div>
    <p>这里的乘法运算会让 $d^{[3]}$ 中所有等于 0 的元素对应的 $a^{[3]}$ 元素归零。在 Python 中，$d^{[3]}$ 是布尔型数组，值为 <strong>true</strong> 和 <strong>false</strong>，乘法运算时 Python 会把 <strong>true</strong> 和 <strong>false</strong> 翻译为 1 和 0。</p>
  </li>
</ol>

<h3 id="17-理解-dropoutunderstanding-dropout">1.7 理解 dropout（Understanding Dropout）</h3>
<h4 id="功能与特点">功能与特点</h4>
<ul>
  <li><strong>dropout</strong> 的功能类似于 $L2$ 正则化，但应用方式不同，且适用于不同的输入范围。</li>
  <li>不同层的 <strong>keep-prob</strong> 可以不同。对于含有较多参数、容易过拟合的层，<strong>keep-prob</strong> 值可以相对较低，如 0.5；对于过拟合程度没那么严重的层，<strong>keep-prob</strong> 值可以高一些，如 0.7；若某一层不必担心过拟合问题，<strong>keep-prob</strong> 可以为 1。通常输入层的 <strong>keep-prob</strong> 接近 1，如 0.9。</li>
</ul>

<h4 id="应用场景与缺点">应用场景与缺点</h4>
<ul>
  <li><strong>dropout</strong> 在计算机视觉领域应用频繁，因为计算机视觉输入量非常大，数据相对不足，容易出现过拟合。</li>
  <li><strong>dropout</strong> 的一大缺点是代价函数 $J$ 不再被明确定义，每次迭代都会随机移除一些节点，难以复查梯度下降的性能。调试时，通常先关闭 <strong>dropout</strong> 函数，将 <strong>keep-prob</strong> 的值设为 1，运行代码确保 $J$ 函数单调递减，然后再打开 <strong>dropout</strong> 函数。</li>
</ul>

<h3 id="113-梯度检验gradient-checking">1.13 梯度检验（Gradient checking）</h3>
<p>梯度检验可用于调试或检验 <strong>backprop</strong> 的实施是否正确，具体步骤如下：</p>
<ol>
  <li>将网络中的所有参数（$W^{[1]}$ 和 $b^{[1]}$……$W^{[l]}$ 和 $b^{[l]}$）转换成一个巨大的向量数据 $\theta$，即将矩阵 $W$ 转换成向量，再进行连接运算。代价函数 $J$ 变为 $\theta$ 的函数 $J(\theta)$。</li>
  <li>同样地，把 $dW^{[1]}$ 和 ${db}^{[1]}$……${dW}^{[l]}$ 和 ${db}^{[l]}$ 转换成一个新的向量 $d\theta$，它与 $\theta$ 具有相同维度。</li>
  <li>为了实施梯度检验，循环执行，对每个 $\theta$ 组成元素计算 $d\theta_{\text{approx}}[i]$ 的值，使用双边误差：
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 示例代码框架，具体实现需要根据实际情况完善
</span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-7</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">theta</span><span class="p">)):</span>
 <span class="n">theta_plus</span> <span class="o">=</span> <span class="n">theta</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
 <span class="n">theta_plus</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">theta_plus</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">epsilon</span>
 <span class="n">theta_minus</span> <span class="o">=</span> <span class="n">theta</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
 <span class="n">theta_minus</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">theta_minus</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">epsilon</span>
 <span class="n">dtheta_approx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="nc">J</span><span class="p">(</span><span class="n">theta_plus</span><span class="p">)</span> <span class="o">-</span> <span class="nc">J</span><span class="p">(</span><span class="n">theta_minus</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">epsilon</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
</ol>

<h2 id="第二周优化算法-optimization-algorithms">第二周：优化算法 (Optimization algorithms)</h2>
<h3 id="21-mini-batch-梯度下降mini-batch-gradient-descent">2.1 Mini-batch 梯度下降（Mini-batch gradient descent）</h3>
<h4 id="训练集分割">训练集分割</h4>
<p>可以把训练集分割为小一点的子集，这些子集被称为 <strong>mini-batch</strong>。假设每个子集中有 1000 个样本，将 $x^{(1)}$ 到 $x^{(1000)}$ 称为第一个子训练集 $X^{{1}}$，$x^{(1001)}$ 到 $x^{(2000)}$ 称为 $X^{{2}}$，以此类推。对 $Y$ 也进行相同处理，得到 $Y^{{1}}$，$Y^{{2}}$……$Y^{{t}}$。</p>

<h4 id="符号说明">符号说明</h4>
<ul>
  <li>之前用上角小括号 $(i)$ 表示训练集里的值，$x^{(i)}$ 是第 $i$ 个训练样本。</li>
  <li>用上角中括号 $[l]$ 表示神经网络的层数，$z^{[l]}$ 表示神经网络中第 $l$ 层的 $z$ 值。</li>
  <li>现在引入大括号 ${t}$ 代表不同的 <strong>mini-batch</strong>，所以有 $X^{{t}}$ 和 $Y^{{t}}$。</li>
</ul>

<h4 id="维数说明">维数说明</h4>
<p>如果 $X^{{1}}$ 是一个有 1000 个样本的训练集，其维数应该是 $(n_{x}, 1000)$，$X^{{2}}$ 等所有子集的维数都是 $(n_{x}, 1000)$，而 $Y^{{t}}$ 的维数都是 $(1, 1000)$。</p>

<h4 id="算法对比">算法对比</h4>
<ul>
  <li><strong>batch</strong> 梯度下降法：同时处理整个训练集。</li>
  <li><strong>mini-batch</strong> 梯度下降法：每次同时处理单个的 <strong>mini-batch</strong> $X^{{t}}$ 和 $Y^{{t}}$，而不是同时处理全部的 $X$ 和 $Y$ 训练集。</li>
</ul>

<h3 id="27-rmsprop">2.7 RMSprop</h3>
<h4 id="算法作用">算法作用</h4>
<p><strong>RMSprop</strong> 可以消除梯度下降中的摆动，包括 <strong>mini-batch</strong> 梯度下降，并允许使用一个更大的学习率 $a$，从而加快算法学习速度。其更新过程会使纵轴方向上摆动较小，横轴方向继续推进。</p>

<h4 id="算法细节">算法细节</h4>
<ul>
  <li>为了避免混淆，采用超参数 $\beta_{2}$（与 <strong>Momentum</strong> 中的 $\beta$ 区分）。</li>
  <li>为确保数值稳定，在分母上加上一个很小的 $\varepsilon$，如 $10^{-8}$。</li>
</ul>

<h3 id="29-学习率衰减learning-rate-decay">2.9 学习率衰减(Learning rate decay)</h3>
<p>学习率衰减可以让学习率随着代数或 <strong>mini-batch</strong> 的增加而递减，常见的学习率衰减公式有：</p>
<ul>
  <li>$a = \frac{1}{1 + decayrate * \text{epoch-num}}a_{0}$（<strong>decay-rate</strong> 为衰减率，<strong>epoch-num</strong> 为代数，$\alpha_{0}$ 为初始学习率）
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 示例代码
</span><span class="n">decay_rate</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">a0</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">epoch_num</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">a</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">decay_rate</span> <span class="o">*</span> <span class="n">epoch_num</span><span class="p">))</span> <span class="o">*</span> <span class="n">a0</span>
</code></pre></div>    </div>
  </li>
  <li>指数衰减：$a = 0.95^{\text{epoch-num}} a_{0}$
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 示例代码
</span><span class="n">a0</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">epoch_num</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">a</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.95</span> <span class="o">**</span> <span class="n">epoch_num</span><span class="p">)</span> <span class="o">*</span> <span class="n">a0</span>
</code></pre></div>    </div>
  </li>
  <li>$a = \frac{k}{\sqrt{\text{epoch-num}}}a_{0}$ 或 $a = \frac{k}{\sqrt{t}}a_{0}$（$t$ 为 <strong>mini-batch</strong> 的数字）</li>
  <li>离散下降：在某些步骤学习率减少一半。</li>
</ul>

<h2 id="第三周-超参数调试batch正则化和程序框架hyperparameter-tuning">第三周 超参数调试、Batch正则化和程序框架（Hyperparameter tuning）</h2>
<h3 id="31-调试处理tuning-process">3.1 调试处理（Tuning process）</h3>
<h4 id="超参数重要性">超参数重要性</h4>
<ul>
  <li>学习速率 $a$ 是需要调试的最重要的超参数。</li>
  <li>其次比较重要的参数有 <strong>Momentum</strong> 参数 $\beta$（默认值 0.9）、<strong>mini-batch</strong> 的大小、隐藏单元数量。</li>
  <li>重要性排第三位的参数有层数、学习率衰减。当应用 <strong>Adam</strong> 算法时，通常将 $\beta_{1}$、$\beta_{2}$ 和 $\varepsilon$ 分别选定为 0.9、0.999 和 $10^{-8}$。</li>
</ul>

<h3 id="35-将-batch-norm-拟合进神经网络fitting-batch-norm-into-a-neural-network">3.5 将 Batch Norm 拟合进神经网络（Fitting Batch Norm into a neural network）</h3>
<h4 id="单层-batch-归一化">单层 Batch 归一化</h4>
<p>在单一隐藏层进行 <strong>Batch</strong> 归一化时，将 $z^{[l]}$ 值进行归一化，此过程由 ${\beta}^{[l]}$ 和 $\gamma^{[l]}$ 两参数控制，得到规范化的 ${\tilde{z}}^{[l]}$，然后将其输入激活函数中得到 $a^{[l]}$，即 $a^{[l]} = g^{[l]}({\tilde{z}}^{[l]})$。</p>

<h4 id="深度网络中的应用">深度网络中的应用</h4>
<p>在深度网络训练中，<strong>Batch</strong> 归一化通常和训练集的 <strong>mini-batch</strong> 一起使用。以第一个 <strong>mini-batch</strong>($X^{{1}}$) 为例，计算 $z^{[1]}$ 后进行 <strong>Batch</strong> 归一化，得到 ${\tilde{z}}^{[1]}$，再应用激活函数得到 $a^{[1]}$，然后用 $w^{[2]}$ 和 $b^{[2]}$ 计算 $z^{[2]}$，重复上述过程。</p>

<h3 id="39-训练一个-softmax-分类器training-a-softmax-classifier">3.9 训练一个 Softmax 分类器（Training a Softmax classifier）</h3>
<h4 id="关键方程">关键方程</h4>
<p>在有 <strong>Softmax</strong> 输出层时，初始化反向传播所需要的关键方程是 $dz^{[l]} = \hat{y} - y$，其中 $\hat{y}$ 和 $y$ 都是 $C×1$ 维向量（$C$ 为分类数），$dz^{[l]}$ 是对 $z^{[l]}$ 损失函数的偏导数（$dz^{[l]} = \frac{\partial J}{\partial z^{[l]}}$）。</p>

<h4 id="编程框架应用">编程框架应用</h4>
<p>在这周的初级练习中，使用深度学习编程框架时，通常只需要专注于把前向传播做对，框架会自动实现反向传播和导数计算。</p>

<h3 id="310-深度学习框架deep-learning-frameworks">3.10 深度学习框架（Deep Learning frameworks）</h3>
<h4 id="选择标准">选择标准</h4>
<ul>
  <li>便于编程：包括神经网络的开发和迭代，以及为产品进行配置。</li>
  <li>运行速度：特别是训练大数据集时，一些框架能更高效地运行和训练神经网络。</li>
  <li>开放性：框架不仅要开源，还要有良好的管理，确保能长时间保持开源。</li>
</ul>

<h4 id="框架作用">框架作用</h4>
<p>深度学习框架通过提供比数值线性代数库更高程度的抽象化，能让开发深度机器学习应用更加高效。</p>

<h1 id="第三门课-结构化机器学习项目structuring-machine-learning-projects">第三门课 结构化机器学习项目（Structuring Machine Learning Projects）</h1>

<h2 id="第一周-机器学习ml策略1ml-strategy1">第一周 机器学习（ML）策略（1）（ML strategy（1））</h2>
<p>此部分文档仅提供目录信息 <code class="language-plaintext highlighter-rouge">[TOC]</code>，暂无详细内容。</p>

<h2 id="第二周机器学习策略2ml-strategy-2">第二周：机器学习策略（2）(ML Strategy (2))</h2>

<h3 id="21-进行误差分析carrying-out-error-analysis">2.1 进行误差分析（Carrying out error analysis）</h3>
<p>在机器学习中，人工统计的错误分析步骤虽看似简单，但能节省大量时间，可迅速明确重要且有希望的方向。具体操作如下：</p>
<ul>
  <li>收集约 100 个错误标记的开发集样本，逐个手动检查，统计其中狗的图片数量。</li>
  <li>若 100 个错误标记样本中只有 5% 是狗，即便完全解决狗的问题，最多也只能使错误率从 10% 降至 9.5%，说明在此问题上投入大量时间可能并非明智之举。</li>
  <li>若 100 个错误标记样本中有 50 张是狗的图片，即占比 50%，那么解决狗的问题可能会使错误率从 10% 降至 5%，值得集中精力解决。</li>
</ul>

<p>错误分析还可同时并行评估多个改善算法的想法，如改善针对狗图的性能、处理猫科动物分类错误、解决模糊图像分类问题等。</p>

<h3 id="22-清除标注错误的数据cleaning-up-incorrectly-labeled-data">2.2 清除标注错误的数据（Cleaning up Incorrectly labeled data）</h3>
<p>监督学习问题的数据由输入 $x$ 和输出标签 $y$ 构成，当发现部分输出标签 $y$ 错误时，可参考以下处理方式：</p>

<h4 id="训练集">训练集</h4>
<p>深度学习算法对训练集中的随机错误具有较强的健壮性。只要标记出错的样本接近随机错误，如标记人员的疏忽或误操作，可不花大量时间修复这些错误。但如果存在系统性错误，如将白色的狗一直标记成猫，会影响分类器的学习效果。</p>

<h4 id="开发集和测试集">开发集和测试集</h4>
<ul>
  <li><strong>判断是否值得修正</strong>：若标记错误严重影响在开发集上评估算法的能力，则应花时间修正；反之，则可不处理。例如，当开发集整体错误率为 10%，其中 6% 的错误来自标记出错，即标记错误导致的错误率为 0.6%，此时标记错误占比较小；若将错误率降至 2%，标记错误仍占 0.6%，则标记错误占比变为 30%，此时修正开发集里的错误标签更有价值。</li>
  <li><strong>修正原则</strong>：
    <ul>
      <li>修正手段应同时作用于开发集和测试集，以确保它们来自相同的分布。</li>
      <li>同时检验算法判断正确和判断错误的样本。只修正算法出错的样本可能会使对算法的偏差估计变大，让算法获得不公平的优势。但由于分类器准确时，判断正确的样本数量远多于判断错误的样本，检查所有判断正确样本的标签耗时过长，通常难以实现。</li>
    </ul>
  </li>
</ul>

<h3 id="23-快速搭建你的第一个系统并进行迭代build-your-first-system-quickly-then-iterate">2.3 快速搭建你的第一个系统，并进行迭代（Build your first system quickly, then iterate）</h3>
<p>对于几乎所有的机器学习程序，可能有多个方向可改善系统，但选择优先处理的方向颇具挑战。建议如下：</p>
<ul>
  <li>快速搭建第一个系统，设立开发集、测试集和评估指标，明确目标。</li>
  <li>构建机器学习系统原型，使用训练集进行训练，观察算法在开发集和测试集上的表现。</li>
  <li>利用偏差方差分析和错误分析，确定下一步优先处理的方向。例如，若错误分析表明大部分错误源于说话人远离麦克风，可集中精力研究远场语音识别技术。</li>
</ul>

<p>初始系统可以是快速且粗糙的实现，其目的是确定偏差方差范围，便于进行错误分析，找出最有希望的改进方向。不过，若在应用程序领域有丰富经验，或该领域有大量可借鉴的学术文献（如人脸识别），则可从现有文献出发，搭建更复杂的系统。</p>

<h3 id="28-多任务学习multi-task-learning">2.8 多任务学习（Multi-task learning）</h3>
<p>与迁移学习中串行学习不同，多任务学习是让单个神经网络同时处理多个任务，期望每个任务能相互促进。</p>

<h4 id="示例">示例</h4>
<p>以研发无人驾驶车辆为例，车辆需同时检测行人、车辆、停车标志和交通灯等物体。输入图像 $x^{(i)}$ 对应 4 个标签，$y^{(i)}$ 为 4×1 向量。将训练集的标签水平堆叠起来，得到 $4×m$ 矩阵 $Y$：
\(Y = \begin{bmatrix}
| &amp; | &amp; | &amp; \ldots &amp; | \\
y^{(1)} &amp; y^{(2)} &amp; y^{(3)} &amp; \ldots &amp; y^{(m)} \\
| &amp; | &amp; | &amp; \ldots &amp; | \\
\end{bmatrix}\)</p>

<p>训练一个神经网络，输入 $x$，输出为四维向量 $\hat y$，分别预测图中是否有行人、车辆、停车标志和交通灯。</p>

<h4 id="与-softmax-回归的区别">与 softmax 回归的区别</h4>
<p>softmax 回归为单个样本分配单个标签，而多任务学习中一张图可以有多个标签。训练一个神经网络处理多个任务比训练多个独立的神经网络分别处理这些任务性能更好。</p>

<h4 id="处理部分标记的情况">处理部分标记的情况</h4>
<p>多任务学习也能处理图像只有部分物体被标记的情况。</p>

<h3 id="210-是否要使用端到端的深度学习whether-to-use-end-to-end-learning">2.10 是否要使用端到端的深度学习？（Whether to use end-to-end learning?）</h3>
<p>以无人驾驶技术为例，非端到端的深度学习方法是将车前方传感器的读数作为输入，先检测附近的车、行人等物体，再通过运动规划软件确定车辆行驶路径，最后由控制算法确定方向盘转角、油门和刹车力度。</p>

<p>虽然端到端的深度学习方法（输入图像直接得出方向盘转角）听起来很有吸引力，但就目前可收集的数据和训练神经网络的能力而言，多步方法可能更具前景。因此，在应用监督学习时，应根据可收集的数据仔细选择要学习的 $x$ 到 $y$ 映射类型。</p>

<p>综上所述，端到端的深度学习有时效果显著，但需谨慎选择使用场景。通过学习本周内容，应能更好地做出优先分配任务的决策，推动机器学习项目的发展。同时，建议完成本周作业，以实践这些理念，确保掌握相关知识。</p>

<h1 id="第四门课-卷积神经网络convolutional-neural-networks">第四门课 卷积神经网络（Convolutional Neural Networks）</h1>

<h2 id="第一周-卷积神经网络foundations-of-convolutional-neural-networks">第一周 卷积神经网络（Foundations of Convolutional Neural Networks）</h2>
<h3 id="12-边缘检测示例edge-detection-example">1.2 边缘检测示例（Edge detection example）</h3>
<p>卷积运算是卷积神经网络最基本的组成部分，以边缘检测作为入门样例。神经网络的前几层检测边缘，后面的层检测物体的部分区域，更靠后的层检测完整的物体。</p>

<p>示例：对6×6的灰度图像（6×6×1矩阵）进行垂直边缘检测，使用3×3的过滤器 $\begin{bmatrix}1 &amp; 0 &amp; -1\ 1 &amp; 0 &amp; -1\ 1 &amp; 0 &amp; -1\end{bmatrix}$ 进行卷积运算。在数学中“$*$”是卷积的标准标志，但在Python中常表示乘法或元素乘法，需特别说明。</p>

<p>卷积运算输出是4×4的矩阵，计算方式如下：</p>
<ul>
  <li>计算第一个元素（4×4左上角）：将3×3过滤器覆盖在输入图像上，进行元素乘法运算，如 $\begin{bmatrix} 3 \times 1 &amp; 0 \times 0 &amp; 1 \times \left(-1 \right) \ 1 \times 1 &amp; 5 \times 0 &amp; 8 \times \left( - 1 \right) \ 2 \times1 &amp; 7 \times 0 &amp; 2 \times \left( - 1 \right) \ \end{bmatrix} = \begin{bmatrix}3 &amp; 0 &amp; - 1 \ 1 &amp; 0 &amp; - 8 \ 2 &amp; 0 &amp; - 2 \\end{bmatrix}$，然后将矩阵元素相加得 $3+1+2+0+0 +0+(-1)+(-8) +(-2)=-5$。</li>
  <li>计算第二个元素：将过滤器向右移动一步，继续元素乘法和相加，$0×1+5×1+7×1+1×0+8×0+2×0+2×(-1)+ 9×(-1)+5×(-1)=-4 $。</li>
</ul>

<h3 id="17-单层卷积网络one-layer-of-a-convolutional-network">1.7 单层卷积网络（One layer of a convolutional network）</h3>
<p>每个过滤器有一个偏差参数，是实数，在代码中表示为1×1×1×$n_{c}^{[l]}$ 的四维向量或四维张量。</p>

<p>卷积有多种标记方法，关于高度、宽度和通道的顺序没有完全统一的标准，课上按高度、宽度和通道损失数量的顺序依次计算。</p>

<h3 id="18-简单卷积网络示例a-simple-convolution-network-example">1.8 简单卷积网络示例（A simple convolution network example）</h3>
<p>假设输入图片大小为39×39×3，即 $n_{H}^{[0]} = n_{W}^{[0]} = 39$，$n_{c}^{[0]} = 3$。</p>
<ul>
  <li>第一层：用3×3的过滤器（$f^{[1]} = 3$），步幅 $s^{[1]} = 1$，padding $p^{[1]} = 0$，10个过滤器。输出激活值为37×37×10，因为 $\frac{39 + 0 - 3}{1} + 1 = 37$，即 $n_{H}^{[1]} = n_{W}^{[1]} = 37$，$n_{c}^{[1]} = 10$。</li>
  <li>第二层：用5×5的过滤器（$f^{\left\lbrack 2 \right\rbrack} = 5$），步幅 $s^{\left\lbrack 2 \right\rbrack} = 2$，padding $p^{\left\lbrack 2 \right\rbrack} = 0$，20个过滤器。输出为17×17×20，即 $n_{H}^{\left\lbrack 2 \right\rbrack} = n_{W}^{\left\lbrack 2 \right\rbrack} = 17$，$n_{c}^{\left\lbrack 2 \right\rbrack} = 20$。</li>
  <li>第三层：用5×5的过滤器（$f^{\left\lbrack 2 \right\rbrack} = 5$），步幅 $s^{\left\lbrack 3 \right\rbrack} = 2$，padding为0，40个过滤器。输出为7×7×40。</li>
</ul>

<h3 id="110-卷积神经网络示例convolutional-neural-network-example">1.10 卷积神经网络示例（Convolutional neural network example）</h3>
<p>假设输入是32×32×3的RGB图片，用于手写体数字识别（识别0 - 9）。</p>
<ul>
  <li><strong>CONV1</strong>：使用5×5的过滤器，步幅1，padding 0，6个过滤器。输出为28×28×6，增加偏差，应用ReLU非线性函数。</li>
  <li><strong>POOL1</strong>：最大池化，过滤器2×2，步幅2，padding 0。输出为14×14×6。</li>
  <li>全连接层：添加一个含84个单元的全连接层（FC4）。</li>
  <li>Softmax层：最后用84个单元填充一个Softmax单元，有10个输出。</li>
</ul>

<p>随着神经网络深度加深，高度 $n_{H}$ 和宽度 $n_{W}$ 通常减少，通道数量增加。常见模式是一个或多个卷积后面跟随一个池化层，然后是几个全连接层，最后是一个Softmax。</p>

<p>激活值形状和大小：输入为32×32×3，激活值 $a^{[0]}$ 有3072维，输入层无参数。池化层和最大池化层无参数，卷积层参数相对较少，许多参数在全连接层。</p>

<h3 id="111-为什么使用卷积why-convolutions">1.11 为什么使用卷积？（Why convolutions?）</h3>
<p>本周学习了卷积神经网络的基本构造模块，可通过编程练习深入理解。下周将展示有效卷积神经网络示例，分析其高效原因，讲解新的计算机视觉应用。</p>

<h2 id="第二周-深度卷积网络实例探究deep-convolutional-models-case-studies">第二周 深度卷积网络：实例探究（Deep convolutional models: case studies）</h2>
<h3 id="21-为什么要进行实例探究why-look-at-case-studies">2.1 为什么要进行实例探究？（Why look at case studies?）</h3>
<p>过去几年计算机视觉研究集中在组合基本构件形成有效卷积神经网络，通过看案例学习是不错的办法。在计算机视觉任务中表现良好的神经网络框架往往适用于其它任务。</p>

<p>后面几节课将介绍经典网络，如LeNet - 5、AlexNet、VGG网络，还有ResNet（残差网络）和Inception神经网络。</p>

<h3 id="29-迁移学习transfer-learning">2.9 迁移学习（Transfer Learning）</h3>
<p>可从网上下载训练好的神经网络开源实现和权重，如ImageNet数据集的网络。去掉原网络的Softmax层，创建自己的Softmax单元，冻结前面层的参数，只训练与新Softmax层有关的参数。不同深度学习框架有不同方式指定是否训练特定层的权重。</p>

<h3 id="211-计算机视觉现状the-state-of-computer-vision">2.11 计算机视觉现状（The state of computer vision）</h3>
<p>计算机视觉研究者致力于在基准测试中表现出色以发表论文，这有助于社区找出有效算法，但有些技巧不适合实际生产应用。</p>

<p>有助于在基准测试中表现出色的技巧：</p>
<ul>
  <li>集成：独立训练几个神经网络，平均它们的输出，但不要平均权重。可提高性能，但会增加运行时间。</li>
  <li>Multi - crop at test time：将数据扩充应用到测试图像，如10 - crop技术，取图片的中心区域和四个角落区域进行裁剪，通过分类器运行，对镜像图像做同样操作。</li>
</ul>

<h2 id="第三周">第三周</h2>
<h3 id="32-特征点检测landmark-detection">3.2 特征点检测（Landmark detection）</h3>
<p>可定义人物的关键特征点，如脸部特征点（假设64个）、人体姿态特征点（如胸部中点、左肩、左肘、腰等，假设32个）。</p>

<p>具体做法：准备卷积网络和特征集，输入人脸图片，输出1或0表示有无人脸，再输出特征点的 $(x,y)$ 坐标。例如，检测脸部特征有129个输出单元（1表示有人脸，64×2 = 128个特征点坐标）。可用于识别脸部表情、实现AR过滤器、脸部扭曲等效果，需要准备标签训练集。</p>

<h3 id="310-候选区域选修region-proposals-optional">3.10 候选区域（选修）（Region proposals (Optional)）</h3>
<p>相关文献：</p>
<ul>
  <li>Joseph Redmon, Ali Farhadi - <a href="https://arxiv.org/abs/1612.08242">YOLO9000: Better, Faster, Stronger</a> (2016)</li>
  <li>Allan Zelener - <a href="https://github.com/allanzelener/YAD2K">YAD2K: Yet Another Darknet 2 Keras</a></li>
  <li>The official YOLO website (<a href="https://pjreddie.com/darknet/yolo/">https://pjreddie.com/darknet/yolo/</a>)</li>
</ul>

<h2 id="第四周">第四周</h2>
<h3 id="47-cnn特征可视化what-are-deep-convnets-learning">4.7 CNN特征可视化（What are deep ConvNets learning?）</h3>
<p>通过可视化例子理解深度卷积网络在学什么，有助于实现神经风格迁移。</p>

<p>以Alexnet为例，从第一层隐藏单元开始，遍历训练集，找到使单元激活最大化的图片或图片块。第一层隐藏单元通常找简单特征，如边缘或颜色阴影。</p>

<h1 id="第五门课-序列模型学习笔记">第五门课 序列模型学习笔记</h1>

<h2 id="第一周-循环序列模型recurrent-neural-networks">第一周 循环序列模型（Recurrent Neural Networks）</h2>
<h3 id="17-对新序列采样sampling-novel-sequences">1.7 对新序列采样（Sampling novel sequences）</h3>
<p>基础的 RNN 结构可用于建立语言模型并进行采样。示例生成的句子如下：</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>And subject of this thou art another this fold.
When besser be my love to me see sabl's.
For whose are ruse of mine eyes heaves.
</code></pre></div></div>
<p>后续将探讨训练 RNN 时的梯度消失问题，以及引入 GRU（门控循环单元）和 LSTM（长期记忆网络模型）。</p>

<h3 id="112-深层循环神经网络deep-rnns">1.12 深层循环神经网络（Deep RNNs）</h3>
<ul>
  <li><strong>激活值计算示例</strong>：以激活值 $a^{\lbrack 2\rbrack &lt;3&gt;}$ 为例，它有两个输入。</li>
  <li><strong>网络层数特点</strong>：与标准神经网络不同，RNN 有三层就算比较深了，因为时间维度会使网络规模增大。</li>
</ul>

<p>从基本的 RNN 网络，到 GRU、LSTM、双向 RNN 以及深层版模型，学完后可构建不错的学习序列的模型。</p>

<h2 id="第二周-词嵌入相关内容">第二周 词嵌入相关内容</h2>
<h3 id="21-词汇表征word-representation">2.1 词汇表征（Word Representation）</h3>
<ul>
  <li><strong>词嵌入与可视化</strong>：学习 300 维的词嵌入后，可用 t - SNE 算法将其可视化到二维空间。相近概念的词嵌入在可视化时会聚集在一起，如 man 和 woman、king 和 queen 等。</li>
  <li><strong>嵌入的含义</strong>：每个单词对应 300 维空间里的一个点，就像被“嵌入”到这个空间中。词嵌入是 NLP 领域重要概念。</li>
</ul>

<h3 id="22-使用词嵌入using-word-embeddings">2.2 使用词嵌入（Using Word Embeddings）</h3>
<ul>
  <li><strong>迁移学习步骤</strong>：
    <ol>
      <li>用词嵌入模型迁移到新的少量标注训练集任务中，用 300 维特征向量代替 10000 维的 one - hot 向量。</li>
      <li>在新任务训练模型时，可选择是否微调词嵌入。若标记数据集小，通常不进行微调。</li>
    </ol>
  </li>
  <li><strong>应用场景</strong>：词嵌入在训练集较小时作用明显，广泛用于命名实体识别、文本摘要、文本解析、指代消解等 NLP 任务，但在语言模型和机器翻译领域应用相对较少。</li>
  <li><strong>与人脸编码的关系</strong>：词嵌入和人脸编码有奇妙关系，人脸识别训练的 Siamese 网络结构与之相关。</li>
</ul>

<h3 id="24-嵌入矩阵embedding-matrix">2.4 嵌入矩阵（Embedding Matrix）</h3>
<ul>
  <li><strong>矩阵定义</strong>：假设词汇表有 10000 个单词，学习的嵌入矩阵 $E$ 是 300×10000 的矩阵，各列代表词汇表中不同单词的向量。</li>
  <li><strong>计算示例</strong>：若“orange”编号是 6257，用 one - hot 向量 $O_{6527}$ 与 $E$ 相乘，可得到“orange”的 300 维嵌入向量。</li>
  <li><strong>实际应用</strong>：实际中，使用专门函数查找矩阵 $E$ 的某列，而非矩阵乘法，如 Keras 中的嵌入层。</li>
</ul>

<h3 id="25-学习词嵌入learning-word-embeddings">2.5 学习词嵌入（Learning Word Embeddings）</h3>
<ul>
  <li><strong>神经网络模型</strong>：将 300 维嵌入向量放入神经网络，经过 softmax 层预测结尾单词。</li>
  <li><strong>固定历史窗口</strong>：使用固定历史窗口（如 4 个单词）可处理任意长度句子，输入维度固定为 1200 维。</li>
  <li><strong>简单上下文模型</strong>：可以用目标词的前一个词或附近一个单词作为上下文来构建神经网络学习词嵌入。</li>
</ul>

<h3 id="26-word2vec">2.6 Word2Vec</h3>
<ul>
  <li><strong>监督学习问题构造</strong>：随机选一个上下文词，在其正负 10 个词距或正负 5 个词距内选目标词，构造监督学习问题来学习词嵌入。</li>
  <li><strong>模型细节</strong>：输入上下文词的 one - hot 向量 $O_{c}$ 与嵌入矩阵 $E$ 相乘得到嵌入向量 $e_{c}$，将其喂入 softmax 单元，预测不同目标词的概率：
  \(Softmax:p\left( t \middle| c \right) = \frac{e^{\theta_{t}^{T}e_{c}}}{\sum_{j = 1}^{10,000}e^{\theta_{j}^{T}e_{c}}}\)</li>
</ul>

<h3 id="27-负采样negative-sampling">2.7 负采样（Negative Sampling）</h3>
<ul>
  <li><strong>训练集生成</strong>：选择一个上下文词和一个目标词作为正样本（标签为 1），再用相同上下文词和从字典中随机选取的词作为负样本（标签为 0）。</li>
  <li><strong>监督学习问题</strong>：输入一对词，预测它们是否一起出现。</li>
  <li><strong>K 值选择</strong>：小数据集 $K$ 选 5 到 20，大数据集 $K$ 选 2 到 5。</li>
</ul>

<h2 id="第三周-序列模型和注意力机制sequence-models--attention-mechanism">第三周 序列模型和注意力机制（Sequence models &amp; Attention mechanism）</h2>
<h3 id="31-基础模型basic-models">3.1 基础模型（Basic Models）</h3>
<ul>
  <li><strong>seq2seq 模型</strong>：本周学习 seq2seq 模型，用于机器翻译、语音识别等。</li>
  <li><strong>编码解码网络</strong>：建立编码网络（RNN 结构，单元可以是 GRU 或 LSTM）对输入序列编码，输出向量；再建立解码网络，以编码网络输出为输入，训练为每次输出一个翻译后的单词。</li>
  <li><strong>图像描述应用</strong>：类似结构可用于图像描述，输入图像输出描述。</li>
</ul>

<h3 id="37-注意力模型直观理解attention-model-intuition">3.7 注意力模型直观理解（Attention Model Intuition）</h3>
<ul>
  <li><strong>模型改进</strong>：在编码解码构架基础上引入注意力模型，使其工作更好。</li>
  <li><strong>模型起源</strong>：注意力模型源于 Dimitri, Bahdanau, Camcrun Cho, Yoshe Bengio 的论文，虽源于机器翻译，但已推广到其他领域。</li>
  <li><strong>示例说明</strong>：用短句举例说明注意力模型思想。</li>
</ul>

<h3 id="310-触发字检测trigger-word-detection">3.10 触发字检测（Trigger Word Detection）</h3>
<ul>
  <li><strong>算法现状</strong>：触发字检测系统文献处于发展阶段，尚无广泛定论的最佳算法。</li>
  <li><strong>简单解决方法</strong>：在输出变回 0 之前多次输出 1，提高 1 与 0 的比例，便于训练。</li>
  <li><strong>学习总结</strong>：总结学习了 RNN（包括 GRU 和 LSTM）、词嵌入、注意力模型等内容。</li>
</ul>

<h3 id="311-结论和致谢conclusion-and-thank-you">3.11 结论和致谢（Conclusion and thank you）</h3>
<ul>
  <li><strong>学习总结</strong>：学完整个专业课程，掌握了神经网络和深度学习、改进深度神经网络、结构化机器学习项目、卷积神经网络和序列模型等知识。</li>
  <li><strong>鼓励与感谢</strong>：深度学习是超能力，恭喜完成课程学习，感谢花费时间和精力学习。</li>
</ul>

  </div><a class="u-url" href="/blog/python/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2023/12/13/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading"></h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name"></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
