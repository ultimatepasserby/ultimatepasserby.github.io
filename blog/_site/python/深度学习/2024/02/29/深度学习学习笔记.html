<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>深度学习学习笔记</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="深度学习学习笔记" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="深度学习学习笔记" />
<meta property="og:description" content="深度学习学习笔记" />
<link rel="canonical" href="http://localhost:4000/blog/python/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2024/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html" />
<meta property="og:url" content="http://localhost:4000/blog/python/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2024/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-02-29T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="深度学习学习笔记" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-02-29T00:00:00+08:00","datePublished":"2024-02-29T00:00:00+08:00","description":"深度学习学习笔记","headline":"深度学习学习笔记","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/python/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2024/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html"},"url":"http://localhost:4000/blog/python/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2024/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/blog/feed.xml" /></head>
<body><a href="/" class="home-link">返回简历主页</a>
<!-- 确保自定义样式被加载 -->
<link rel="stylesheet" href="/blog/assets/css/main.scss"><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">深度学习学习笔记</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2024-02-29T00:00:00+08:00" itemprop="datePublished">Feb 29, 2024
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="深度学习学习笔记">深度学习学习笔记</h1>

<h2 id="一神经网络基础-neural-network-basics">一、神经网络基础 (Neural Network Basics)</h2>

<h3 id="11-神经网络的组成">1.1 神经网络的组成</h3>
<ul>
  <li><strong>神经元</strong>：模拟生物神经元，接收输入信号并通过激活函数输出</li>
  <li><strong>权重 (Weights)</strong>：控制输入信号的重要性</li>
  <li><strong>偏置 (Bias)</strong>：调整神经元的激活阈值</li>
  <li><strong>激活函数</strong>：引入非线性，常见类型：
    <ul>
      <li>Sigmoid: σ(x) = 1/(1+e⁻ˣ) （用于二分类输出层）</li>
      <li>ReLU: f(x)=max(0,x) （隐藏层首选）</li>
      <li>Softmax: 多分类输出层</li>
    </ul>
  </li>
</ul>

<h4 id="案例房价预测">案例：房价预测</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 单神经元实现
</span><span class="k">def</span> <span class="nf">predict_price</span><span class="p">(</span><span class="n">area</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">return</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">area</span> <span class="o">*</span> <span class="n">weight</span> <span class="o">+</span> <span class="n">bias</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="12-反向传播算法">1.2 反向传播算法</h3>
<ol>
  <li>前向传播计算损失</li>
  <li>计算损失函数对权重的梯度 ∂L/∂W</li>
  <li>使用链式法则逐层回传梯度</li>
  <li>更新参数：W = W - α*(∂L/∂W)</li>
</ol>

<h4 id="案例线性回归梯度计算">案例：线性回归梯度计算</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 简化版梯度下降
</span><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">X</span> <span class="o">*</span> <span class="n">W</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">dW</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="p">(</span><span class="n">predictions</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
    <span class="n">db</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">predictions</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">W</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dW</span>
    <span class="n">b</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">db</span>
</code></pre></div></div>

<hr />

<h2 id="二浅层神经网络-shallow-neural-networks">二、浅层神经网络 (Shallow Neural Networks)</h2>

<h3 id="21-网络结构">2.1 网络结构</h3>
<ul>
  <li>输入层 → 隐藏层 → 输出层</li>
  <li>隐藏层维度选择：通常为输入特征数的 1.5-2 倍</li>
</ul>

<h3 id="22-激活函数对比">2.2 激活函数对比</h3>
<p>| 函数      | 优点                     | 缺点                     | 使用场景           |
|———–|————————–|————————–|——————–|
| ReLU      | 计算快，缓解梯度消失     | 神经元可能”死亡”        | 隐藏层首选         |
| Leaky ReLU| 解决神经元死亡问题       | 负值区域响应较弱         | 需改进ReLU时使用   |
| Tanh      | 输出零中心化             | 梯度消失问题依然存在     | 二分类输出层       |</p>

<h3 id="23-损失函数">2.3 损失函数</h3>
<ul>
  <li><strong>交叉熵损失</strong>：用于分类任务
\(L = -\frac{1}{N}\sum_{i=1}^N [y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]\)</li>
  <li><strong>均方误差</strong>：用于回归任务
\(MSE = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2\)</li>
</ul>

<h4 id="案例二分类交叉熵计算">案例：二分类交叉熵计算</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-15</span>  <span class="c1"># 防止log(0)
</span>    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">y_true</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre></div></div>

<hr />

<h2 id="三深层神经网络-deep-neural-networks">三、深层神经网络 (Deep Neural Networks)</h2>

<h3 id="31-网络深度选择">3.1 网络深度选择</h3>
<ul>
  <li>图像识别：至少5层</li>
  <li>NLP任务：10-24层Transformer</li>
  <li>过拟合预防：增加深度时配合使用正则化</li>
</ul>

<h3 id="32-权重初始化">3.2 权重初始化</h3>
<ul>
  <li><strong>Xavier初始化</strong>：适合tanh激活
\(W \sim \mathcal{N}(0, \sqrt{\frac{1}{n_{in}}})\)</li>
  <li><strong>He初始化</strong>：适合ReLU激活
\(W \sim \mathcal{N}(0, \sqrt{\frac{2}{n_{in}}})\)</li>
</ul>

<h4 id="案例he初始化实现">案例：He初始化实现</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">he_initialization</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">n_in</span><span class="p">):</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mf">2.</span> <span class="o">/</span> <span class="n">n_in</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="o">*</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">std</span>
</code></pre></div></div>

<h3 id="33-批归一化-batch-normalization">3.3 批归一化 (Batch Normalization)</h3>
<ol>
  <li>计算均值 μ = (1/m)Σx_i</li>
  <li>计算方差 σ² = (1/m)Σ(x_i-μ)²</li>
  <li>归一化 x_norm = (x-μ)/√(σ²+ε)</li>
  <li>缩放平移 y = γ*x_norm + β</li>
</ol>

<h4 id="案例批归一化层实现">案例：批归一化层实现</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BatchNorm</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">var</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">x_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">eps</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">x_norm</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span>
</code></pre></div></div>

<hr />

<h2 id="四超参数调优-hyperparameter-tuning">四、超参数调优 (Hyperparameter Tuning)</h2>

<h3 id="41-学习率选择">4.1 学习率选择</h3>
<ul>
  <li>典型值范围：0.001, 0.01, 0.1</li>
  <li>学习率衰减策略：
    <ul>
      <li>指数衰减：α = α₀ * 0.95^epoch</li>
      <li>离散下降：每5个epoch减少50%</li>
    </ul>
  </li>
</ul>

<h3 id="42-优化器对比">4.2 优化器对比</h3>
<p>| 优化器       | 特点                          | 适用场景               |
|————–|——————————-|————————|
| Adam         | 自适应学习率，收敛快          | 大多数场景首选         |
| RMSprop      | 自适应学习率，适合RNN         | 循环神经网络           |
| Momentum     | 加入动量项加速SGD             | 有局部最优问题的场景   |</p>

<h4 id="案例adam优化器实现">案例：Adam优化器实现</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AdamOptimizer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">m</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">v</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="n">self</span><span class="p">.</span><span class="n">beta1</span> <span class="o">=</span> <span class="n">beta1</span>
        <span class="n">self</span><span class="p">.</span><span class="n">beta2</span> <span class="o">=</span> <span class="n">beta2</span>
    
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">params</span><span class="p">)):</span>
            <span class="n">self</span><span class="p">.</span><span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">beta1</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">beta1</span><span class="p">)</span><span class="o">*</span><span class="n">grads</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">beta2</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">beta2</span><span class="p">)</span><span class="o">*</span><span class="n">grads</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
            <span class="n">m_hat</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">beta1</span><span class="o">**</span><span class="n">self</span><span class="p">.</span><span class="n">t</span><span class="p">)</span>
            <span class="n">v_hat</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">beta2</span><span class="o">**</span><span class="n">self</span><span class="p">.</span><span class="n">t</span><span class="p">)</span>
            <span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">self</span><span class="p">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">m_hat</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">v_hat</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>
</code></pre></div></div>

<hr />

<h2 id="五卷积神经网络-cnn">五、卷积神经网络 (CNN)</h2>

<h3 id="51-卷积层参数计算">5.1 卷积层参数计算</h3>
<ul>
  <li>输出尺寸：O = (W - F + 2P)/S + 1
    <ul>
      <li>W: 输入尺寸</li>
      <li>F: 卷积核大小</li>
      <li>P: 填充</li>
      <li>S: 步长</li>
    </ul>
  </li>
</ul>

<h3 id="52-经典网络架构">5.2 经典网络架构</h3>
<p>| 网络       | 年份 | 特点                          |
|————|——|——————————-|
| LeNet      | 1998 | 最早卷积网络，5层结构         |
| VGG16      | 2014 | 全部使用3x3卷积核，16层       |
| ResNet     | 2015 | 引入残差连接，解决梯度消失     |</p>

<h4 id="案例resnet残差模块">案例：ResNet残差模块</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ResidualBlock</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">filters</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="nc">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="nc">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">shortcut</span> <span class="o">=</span> <span class="n">X</span>
        <span class="n">X</span> <span class="o">=</span> <span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">conv1</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv2</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">+=</span> <span class="n">shortcut</span>  <span class="c1"># 残差连接
</span>        <span class="k">return</span> <span class="nf">relu</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div></div>

<hr />

<h2 id="六序列模型-sequence-models">六、序列模型 (Sequence Models)</h2>

<h3 id="61-rnn变体对比">6.1 RNN变体对比</h3>
<p>| 模型       | 遗忘机制 | 门控结构 | 适用场景           |
|————|———-|———-|——————–|
| Vanilla RNN| 无       | 无       | 简单序列任务       |
| LSTM       | 有       | 输入/遗忘/输出门 | 长序列依赖         |
| GRU        | 有       | 重置/更新门      | 需要平衡效果与效率 |</p>

<h3 id="62-注意力机制">6.2 注意力机制</h3>
<ol>
  <li>计算Query-Key相似度：e_t = Query·Key_t</li>
  <li>计算注意力权重：α_t = softmax(e_t)</li>
  <li>加权求和Value向量：c = Σα_t·Value_t</li>
</ol>

<h4 id="案例注意力实现">案例：注意力实现</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="n">d_k</span> <span class="o">=</span> <span class="n">query</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</code></pre></div></div>

<hr />

<h2 id="七实践建议-practical-tips">七、实践建议 (Practical Tips)</h2>

<h3 id="71-数据预处理">7.1 数据预处理</h3>
<ol>
  <li>图像：[0,255] → [-1,1] 或 [0,1]</li>
  <li>文本：构建词表 → 分词 → Word2Vec嵌入</li>
  <li>标准化公式：x’ = (x - μ)/(σ + ε)</li>
</ol>

<h3 id="72-正则化技术">7.2 正则化技术</h3>
<ul>
  <li>L2正则化：λΣw²</li>
  <li>Dropout：训练时随机失活神经元</li>
  <li>早停法：监控验证集损失停止训练</li>
</ul>

<h4 id="案例dropout实现">案例：Dropout实现</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">dropout</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">drop_prob</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">training</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">X</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="o">*</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">drop_prob</span>
    <span class="k">return</span> <span class="n">X</span> <span class="o">*</span> <span class="n">mask</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">drop_prob</span><span class="p">)</span>  <span class="c1"># 反向缩放
</span></code></pre></div></div>

  </div><a class="u-url" href="/blog/python/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2024/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading"></h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name"></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
