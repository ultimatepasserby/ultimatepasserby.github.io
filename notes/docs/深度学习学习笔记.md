---
title: "深度学习学习笔记"
date: 2025-06-15  # 可选，手动指定日期
layout: default      # 可选，指定布局（如 `post` 或 `default`）
categories: [深度学习, Python]  # 可选，添加分类
---
# 深度学习学习笔记

## 一、神经网络基础 (Neural Network Basics)

### 1.1 神经网络的组成
- **神经元**：模拟生物神经元，接收输入信号并通过激活函数输出
- **权重 (Weights)**：控制输入信号的重要性
- **偏置 (Bias)**：调整神经元的激活阈值
- **激活函数**：引入非线性，常见类型：
  - Sigmoid: σ(x) = 1/(1+e⁻ˣ) （用于二分类输出层）
  - ReLU: f(x)=max(0,x) （隐藏层首选）
  - Softmax: 多分类输出层

#### 案例：房价预测
```python
# 单神经元实现
def predict_price(area, weight=0.5, bias=10):
    return ReLU(area * weight + bias)
```

### 1.2 反向传播算法
1. 前向传播计算损失
2. 计算损失函数对权重的梯度 ∂L/∂W
3. 使用链式法则逐层回传梯度
4. 更新参数：W = W - α*(∂L/∂W)

#### 案例：线性回归梯度计算
```python
# 简化版梯度下降
def gradient_descent(X, y, learning_rate=0.01):
    m = len(y)
    predictions = X * W + b
    dW = (1/m) * np.dot(X.T, (predictions - y))
    db = (1/m) * np.sum(predictions - y)
    W -= learning_rate * dW
    b -= learning_rate * db
```

---

## 二、浅层神经网络 (Shallow Neural Networks)

### 2.1 网络结构
- 输入层 → 隐藏层 → 输出层
- 隐藏层维度选择：通常为输入特征数的 1.5-2 倍

### 2.2 激活函数对比
| 函数      | 优点                     | 缺点                     | 使用场景           |
|-----------|--------------------------|--------------------------|--------------------|
| ReLU      | 计算快，缓解梯度消失     | 神经元可能"死亡"        | 隐藏层首选         |
| Leaky ReLU| 解决神经元死亡问题       | 负值区域响应较弱         | 需改进ReLU时使用   |
| Tanh      | 输出零中心化             | 梯度消失问题依然存在     | 二分类输出层       |

### 2.3 损失函数
- **交叉熵损失**：用于分类任务
  $$ L = -\frac{1}{N}\sum_{i=1}^N [y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)] $$
- **均方误差**：用于回归任务
  $$ MSE = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2 $$

#### 案例：二分类交叉熵计算
```python
def binary_cross_entropy(y_true, y_pred):
    epsilon = 1e-15  # 防止log(0)
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
```

---

## 三、深层神经网络 (Deep Neural Networks)

### 3.1 网络深度选择
- 图像识别：至少5层
- NLP任务：10-24层Transformer
- 过拟合预防：增加深度时配合使用正则化

### 3.2 权重初始化
- **Xavier初始化**：适合tanh激活
  $$ W \sim \mathcal{N}(0, \sqrt{\frac{1}{n_{in}}}) $$
- **He初始化**：适合ReLU激活
  $$ W \sim \mathcal{N}(0, \sqrt{\frac{2}{n_{in}}}) $$

#### 案例：He初始化实现
```python
def he_initialization(shape, n_in):
    std = np.sqrt(2. / n_in)
    return np.random.randn(*shape) * std
```

### 3.3 批归一化 (Batch Normalization)
1. 计算均值 μ = (1/m)Σx_i
2. 计算方差 σ² = (1/m)Σ(x_i-μ)²
3. 归一化 x_norm = (x-μ)/√(σ²+ε)
4. 缩放平移 y = γ*x_norm + β

#### 案例：批归一化层实现
```python
class BatchNorm:
    def __init__(self, dim, eps=1e-5):
        self.gamma = np.ones(dim)
        self.beta = np.zeros(dim)
        self.eps = eps
    
    def forward(self, x):
        mu = np.mean(x, axis=0)
        var = np.var(x, axis=0)
        x_norm = (x - mu) / np.sqrt(var + self.eps)
        return self.gamma * x_norm + self.beta
```

---

## 四、超参数调优 (Hyperparameter Tuning)

### 4.1 学习率选择
- 典型值范围：0.001, 0.01, 0.1
- 学习率衰减策略：
  - 指数衰减：α = α₀ * 0.95^epoch
  - 离散下降：每5个epoch减少50%

### 4.2 优化器对比
| 优化器       | 特点                          | 适用场景               |
|--------------|-------------------------------|------------------------|
| Adam         | 自适应学习率，收敛快          | 大多数场景首选         |
| RMSprop      | 自适应学习率，适合RNN         | 循环神经网络           |
| Momentum     | 加入动量项加速SGD             | 有局部最优问题的场景   |

#### 案例：Adam优化器实现
```python
class AdamOptimizer:
    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999):
        self.m = [np.zeros_like(p) for p in params]
        self.v = [np.zeros_like(p) for p in params]
        self.t = 0
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
    
    def update(self, params, grads):
        self.t += 1
        for i in range(len(params)):
            self.m[i] = self.beta1*self.m[i] + (1-self.beta1)*grads[i]
            self.v[i] = self.beta2*self.v[i] + (1-self.beta2)*grads[i]**2
            m_hat = self.m[i] / (1 - self.beta1**self.t)
            v_hat = self.v[i] / (1 - self.beta2**self.t)
            params[i] -= self.lr * m_hat / (np.sqrt(v_hat) + 1e-8)
```

---

## 五、卷积神经网络 (CNN)

### 5.1 卷积层参数计算
- 输出尺寸：O = (W - F + 2P)/S + 1
  - W: 输入尺寸
  - F: 卷积核大小
  - P: 填充
  - S: 步长

### 5.2 经典网络架构
| 网络       | 年份 | 特点                          |
|------------|------|-------------------------------|
| LeNet      | 1998 | 最早卷积网络，5层结构         |
| VGG16      | 2014 | 全部使用3x3卷积核，16层       |
| ResNet     | 2015 | 引入残差连接，解决梯度消失     |

#### 案例：ResNet残差模块
```python
class ResidualBlock:
    def __init__(self, filters):
        self.conv1 = Conv2D(filters, kernel_size=3, padding='same')
        self.conv2 = Conv2D(filters, kernel_size=3, padding='same')
        
    def forward(self, X):
        shortcut = X
        X = relu(self.conv1(X))
        X = self.conv2(X)
        X += shortcut  # 残差连接
        return relu(X)
```

---

## 六、序列模型 (Sequence Models)

### 6.1 RNN变体对比
| 模型       | 遗忘机制 | 门控结构 | 适用场景           |
|------------|----------|----------|--------------------|
| Vanilla RNN| 无       | 无       | 简单序列任务       |
| LSTM       | 有       | 输入/遗忘/输出门 | 长序列依赖         |
| GRU        | 有       | 重置/更新门      | 需要平衡效果与效率 |

### 6.2 注意力机制
1. 计算Query-Key相似度：e_t = Query·Key_t
2. 计算注意力权重：α_t = softmax(e_t)
3. 加权求和Value向量：c = Σα_t·Value_t

#### 案例：注意力实现
```python
def attention(query, key, value):
    d_k = query.shape[-1]
    scores = np.matmul(query, key.transpose(-2, -1)) / np.sqrt(d_k)
    weights = softmax(scores)
    return np.matmul(weights, value)
```

---

## 七、实践建议 (Practical Tips)

### 7.1 数据预处理
1. 图像：[0,255] → [-1,1] 或 [0,1]
2. 文本：构建词表 → 分词 → Word2Vec嵌入
3. 标准化公式：x' = (x - μ)/(σ + ε)

### 7.2 正则化技术
- L2正则化：λΣw²
- Dropout：训练时随机失活神经元
- 早停法：监控验证集损失停止训练

#### 案例：Dropout实现
```python
def dropout(X, drop_prob, training=True):
    if not training:
        return X
    mask = np.random.rand(*X.shape) > drop_prob
    return X * mask / (1 - drop_prob)  # 反向缩放
```